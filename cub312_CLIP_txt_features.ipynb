{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ef02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import lr_scheduler\n",
    "import timm\n",
    "from torch import nn \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPVisionModel, CLIPTextModel, CLIPTokenizer, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca78807",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUBattributeclasses=['has a curved (up or down) bill shape',\n",
    "'has a dagger bill shape',\n",
    "'has a hooked bill shape',\n",
    "'has a needle bill shape',\n",
    "'has a hooked seabird bill shape',\n",
    "'has a spatulate bill shape',\n",
    "'has a all-purpose bill shape',\n",
    "'has a cone bill shape',\n",
    "'has a specialized bill shape',\n",
    "'has a blue wing color',\n",
    "'has a brown wing color',\n",
    "'has a iridescent wing color',\n",
    "'has a purple wing color',\n",
    "'has a rufous wing color',\n",
    "'has a grey wing color',\n",
    "'has a yellow wing color',\n",
    "'has an olive wing color',\n",
    "'has a green wing color',\n",
    "'has a pink wing color',\n",
    "'has an orange wing color',\n",
    "'has a black wing color',\n",
    "'has a white wing color',\n",
    "'has a red wing color',\n",
    "'has a buff wing color',\n",
    "'has a blue upperparts color',\n",
    "'has a brown upperparts color',\n",
    "'has an iridescent upperparts color',\n",
    "'has a purple upperparts color',\n",
    "'has a rufous upperparts color',\n",
    "'has a grey upperparts color',\n",
    "'has a yellow upperparts color',\n",
    "'has a olive upperparts color',\n",
    "'has a green upperparts color',\n",
    "'has a pink upperparts color',\n",
    "'has an orange upperparts color',\n",
    "'has a black upperparts color',\n",
    "'has a white upperparts color',\n",
    "'has a red upperparts color',\n",
    "'has a buff upperparts color',\n",
    "'has a blue underparts color',\n",
    "'has a brown underparts color',\n",
    "'has an iridescent underparts color',\n",
    "'has a purple underparts color',\n",
    "'has a rufous underparts color',\n",
    "'has a grey underparts color',\n",
    "'has a yellow underparts color',\n",
    "'has an olive underparts color',\n",
    "'has a green underparts color',\n",
    "'has a pink underparts color',\n",
    "'has an orange underparts color',\n",
    "'has a black underparts color',\n",
    "'has a white underparts color',\n",
    "'has a red underparts color',\n",
    "'has a buff underparts color',\n",
    "'has a solid breast pattern',\n",
    "'has a spotted breast pattern',\n",
    "'has a striped breast pattern',\n",
    "'has a multi-colored breast pattern',\n",
    "'has a blue back color',\n",
    "'has a brown back color',\n",
    "'has a iridescent back color',\n",
    "'has a purple back color',\n",
    "'has a rufous back color',\n",
    "'has a grey back color',\n",
    "'has a yellow back color',\n",
    "'has a olive back color',\n",
    "'has a green back color',\n",
    "'has a pink back color',\n",
    "'has an orange back color',\n",
    "'has a black back color',\n",
    "'has a white back color',\n",
    "'has a red back color',\n",
    "'has a buff back color',\n",
    "'has a forked tail', \n",
    "'has a rounded tail',\n",
    "'has a notched tail',\n",
    "'has a fan-shaped tail',\n",
    "'has a pointed tail',\n",
    "'has a squared tail',\n",
    "'has a blue upper tail',\n",
    "'has a brown upper tail',\n",
    "'has a iridescent upper tail',\n",
    "'has a purple upper tail',\n",
    "'has a rufous upper tail',\n",
    "'has a grey upper tail',\n",
    "'has a yellow upper tail',\n",
    "'has a olive upper tail',\n",
    "'has a green upper tail',\n",
    "'has a pink upper tail',\n",
    "'has a orange upper tail',\n",
    "'has a black upper tail',\n",
    "'has a white upper tail',\n",
    "'has a red upper tail',\n",
    "'has a buff upper tail',\n",
    "'has a spotted head pattern',\n",
    "'has a malar head pattern',\n",
    "'has a crested head pattern',\n",
    "'has a masked head pattern',\n",
    "'has a unique head pattern',\n",
    "'has a eyebrow head pattern',\n",
    "'has a eyering head pattern',\n",
    "'has a plain head pattern',\n",
    "'has a eyeline head pattern',\n",
    "'has a striped head pattern',\n",
    "'has a capped head pattern',\n",
    "'has a blue breast color',\n",
    "'has a brown breast color',\n",
    "'has a iridescent breast color',\n",
    "'has a purple breast color',\n",
    "'has a rufous breast color',\n",
    "'has a grey breast color',\n",
    "'has a yellow breast color',\n",
    "'has a olive breast color',\n",
    "'has a green breast color',\n",
    "'has a pink breast color',\n",
    "'has an orange breast color',\n",
    "'has a black breast color',\n",
    "'has a white breast color',\n",
    "'has a red breast color',\n",
    "'has a buff breast color',\n",
    "'has a blue throat color',\n",
    "'has a brown throat color',\n",
    "'has a iridescent throat color',\n",
    "'has a purple throat color',\n",
    "'has a rufous throat color',\n",
    "'has a grey throat color',\n",
    "'has a yellow throat color',\n",
    "'has a olive throat color',\n",
    "'has a green throat color',\n",
    "'has a pink throat color',\n",
    "'has a orange throat color',\n",
    "'has a black throat color',\n",
    "'has a white throat color',\n",
    "'has a red throat color',\n",
    "'has a buff throat color',\n",
    "'has a blue eye color',\n",
    "'has a brown eye color',\n",
    "'has a purple eye color',\n",
    "'has a rufous eye color',\n",
    "'has a grey eye color',\n",
    "'has a yellow eye color',\n",
    "'has a olive eye color',\n",
    "'has a green eye color',\n",
    "'has a pink eye color',\n",
    "'has a orange eye color',\n",
    "'has a black eye color',\n",
    "'has a white eye color',\n",
    "'has a red eye color',\n",
    "'has a buff eye color',\n",
    "'has a about the same as head bill length',\n",
    "'has a longer than head bill length',\n",
    "'has a shorter than head bill length',\n",
    "'has a blue forehead color',\n",
    "'has a brown forehead color',\n",
    "'has a iridescent forehead color',\n",
    "'has a purple forehead color',\n",
    "'has a rufous forehead color',\n",
    "'has a grey forehead color',\n",
    "'has a yellow forehead color',\n",
    "'has a olive forehead color',\n",
    "'has a green forehead color',\n",
    "'has a pink forehead color',\n",
    "'has a orange forehead color',\n",
    "'has a black forehead color',\n",
    "'has a white forehead color',\n",
    "'has a red forehead color',\n",
    "'has a buff forehead color',\n",
    "'has a blue under tail color',\n",
    "'has a brown under tail color',\n",
    "'has a iridescent under tail color',\n",
    "'has a purple under tail color',\n",
    "'has a rufous under tail color',\n",
    "'has a grey under tail color',\n",
    "'has a yellow under tail color',\n",
    "'has a olive under tail color',\n",
    "'has a green under tail color',\n",
    "'has a pink under tail color',\n",
    "'has a orange under tail color',\n",
    "'has a black under tail color',\n",
    "'has a white under tail color',\n",
    "'has a red under tail color',\n",
    "'has a buff under tail color',\n",
    "'has a blue nape color',\n",
    "'has a brown nape color',\n",
    "'has a iridescent nape color',\n",
    "'has a purple nape color',\n",
    "'has a rufous nape color',\n",
    "'has a grey nape color',\n",
    "'has a yellow nape color',\n",
    "'has a olive nape color',\n",
    "'has a green nape color',\n",
    "'has a pink nape color',\n",
    "'has a orange nape color',\n",
    "'has a black nape color',\n",
    "'has a white nape color',\n",
    "'has a red nape color',\n",
    "'has a buff nape color',\n",
    "'has a blue belly color',\n",
    "'has a brown belly color',\n",
    "'has a iridescent belly color',\n",
    "'has a purple belly color',\n",
    "'has a rufous belly color',\n",
    "'has a grey belly color',\n",
    "'has a yellow belly color',\n",
    "'has a olive belly color',\n",
    "'has a green belly color',\n",
    "'has a pink belly color',\n",
    "'has a orange belly color',\n",
    "'has a black belly color',\n",
    "'has a white belly color',\n",
    "'has a red belly color',\n",
    "'has a buff belly color',\n",
    "'has a rounded-wings wing shape',\n",
    "'has a pointed-wings wing shape',\n",
    "'has a broad-wings wing shape',\n",
    "'has a tapered-wings wing shape',\n",
    "'has a long-wings wing shape',\n",
    "'has a large size',\n",
    "'has a small size',\n",
    "'has a very large size',\n",
    "'has a medium size',\n",
    "'has a very small size',\n",
    "'has a upright-perching water-like shape',\n",
    "'has a chicken-like-marsh shape',\n",
    "'has a long-legged-like shape',\n",
    "'has a duck-like shape',\n",
    "'has a owl-like shape',\n",
    "'has a gull-like shape',\n",
    "'has a hummingbird-like shape',\n",
    "'has a pigeon-like shape',\n",
    "'has a tree-clinging-like shape',\n",
    "'has a hawk-like shape',\n",
    "'has a sandpiper-like shape',\n",
    "'has a upland-ground-like shape',\n",
    "'has a swallow-like shape',\n",
    "'has a perching-like shape',\n",
    "'has a solid back pattern',\n",
    "'has a spotted back pattern',\n",
    "'has a striped back pattern',\n",
    "'has a multi-colored back pattern',\n",
    "'has a solid tail pattern',\n",
    "'has a spotted tail pattern',\n",
    "'has a striped tail pattern',\n",
    "'has a multi-colored tail pattern',\n",
    "'has a solid belly pattern',\n",
    "'has a spotted belly pattern',\n",
    "'has a striped belly pattern',\n",
    "'has a multi-colored belly pattern',\n",
    "'has a blue primary color',\n",
    "'has a brown primary color',\n",
    "'has a iridescent primary color',\n",
    "'has a purple primary color',\n",
    "'has a rufous primary color',\n",
    "'has a grey primary color',\n",
    "'has a yellow primary color',\n",
    "'has a olive primary color',\n",
    "'has a green primary color',\n",
    "'has a pink primary color',\n",
    "'has a orange primary color',\n",
    "'has a black primary color',\n",
    "'has a white primary color',\n",
    "'has a red primary color',\n",
    "'has a buff primary color',\n",
    "'has a blue leg color',\n",
    "'has a brown leg color',\n",
    "'has a iridescent leg color',\n",
    "'has a purple leg color',\n",
    "'has a rufous leg color',\n",
    "'has a grey leg color',\n",
    "'has a yellow leg color',\n",
    "'has a olive leg color',\n",
    "'has a green leg color',\n",
    "'has a pink leg color',\n",
    "'has a orange leg color',\n",
    "'has a black leg color',\n",
    "'has a white leg color',\n",
    "'has a red leg color',\n",
    "'has a buff leg color',\n",
    "'has a blue bill color',\n",
    "'has a brown bill color',\n",
    "'has a iridescent bill color',\n",
    "'has a purple bill color',\n",
    "'has a rufous bill color',\n",
    "'has a grey bill color',\n",
    "'has a yellow bill color',\n",
    "'has a olive bill color', \n",
    "'has a green bill color',\n",
    "'has a pink bill color',\n",
    "'has a orange bill color',\n",
    "'has a black bill color',\n",
    "'has a white bill color',\n",
    "'has a red bill color',\n",
    "'has a buff bill color',\n",
    "'has a blue crown color',\n",
    "'has a brown crown color',\n",
    "'has a iridescent crown color',\n",
    "'has a purple crown color',\n",
    "'has a rufous crown color',\n",
    "'has a grey crown color',\n",
    "'has a yellow crown color',\n",
    "'has a olive crown color',\n",
    "'has a green crown color',\n",
    "'has a pink crown color',\n",
    "'has a orange crown color',\n",
    "'has a black crown color',\n",
    "'has a white crown color',\n",
    "'has a red crown color',\n",
    "'has a buff crown color',\n",
    "'has a solid wing pattern',\n",
    "'has a spotted wing pattern',\n",
    "'has a striped wing pattern',\n",
    "'has a multi-colored wing pattern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6392a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has a curved (up or down) bill shape\n",
      "has a dagger bill shape\n",
      "has a hooked bill shape\n",
      "has a needle bill shape\n",
      "has a hooked seabird bill shape\n",
      "has a spatulate bill shape\n",
      "has a all-purpose bill shape\n",
      "has a cone bill shape\n",
      "has a specialized bill shape\n",
      "has a blue wing color\n",
      "has a brown wing color\n",
      "has a iridescent wing color\n",
      "has a purple wing color\n",
      "has a rufous wing color\n",
      "has a grey wing color\n",
      "has a yellow wing color\n",
      "has an olive wing color\n",
      "has a green wing color\n",
      "has a pink wing color\n",
      "has an orange wing color\n",
      "has a black wing color\n",
      "has a white wing color\n",
      "has a red wing color\n",
      "has a buff wing color\n",
      "has a blue upperparts color\n",
      "has a brown upperparts color\n",
      "has an iridescent upperparts color\n",
      "has a purple upperparts color\n",
      "has a rufous upperparts color\n",
      "has a grey upperparts color\n",
      "has a yellow upperparts color\n",
      "has a olive upperparts color\n",
      "has a green upperparts color\n",
      "has a pink upperparts color\n",
      "has an orange upperparts color\n",
      "has a black upperparts color\n",
      "has a white upperparts color\n",
      "has a red upperparts color\n",
      "has a buff upperparts color\n",
      "has a blue underparts color\n",
      "has a brown underparts color\n",
      "has an iridescent underparts color\n",
      "has a purple underparts color\n",
      "has a rufous underparts color\n",
      "has a grey underparts color\n",
      "has a yellow underparts color\n",
      "has an olive underparts color\n",
      "has a green underparts color\n",
      "has a pink underparts color\n",
      "has an orange underparts color\n",
      "has a black underparts color\n",
      "has a white underparts color\n",
      "has a red underparts color\n",
      "has a buff underparts color\n",
      "has a solid breast pattern\n",
      "has a spotted breast pattern\n",
      "has a striped breast pattern\n",
      "has a multi-colored breast pattern\n",
      "has a blue back color\n",
      "has a brown back color\n",
      "has a iridescent back color\n",
      "has a purple back color\n",
      "has a rufous back color\n",
      "has a grey back color\n",
      "has a yellow back color\n",
      "has a olive back color\n",
      "has a green back color\n",
      "has a pink back color\n",
      "has an orange back color\n",
      "has a black back color\n",
      "has a white back color\n",
      "has a red back color\n",
      "has a buff back color\n",
      "has a forked tail\n",
      "has a rounded tail\n",
      "has a notched tail\n",
      "has a fan-shaped tail\n",
      "has a pointed tail\n",
      "has a squared tail\n",
      "has a blue upper tail\n",
      "has a brown upper tail\n",
      "has a iridescent upper tail\n",
      "has a purple upper tail\n",
      "has a rufous upper tail\n",
      "has a grey upper tail\n",
      "has a yellow upper tail\n",
      "has a olive upper tail\n",
      "has a green upper tail\n",
      "has a pink upper tail\n",
      "has a orange upper tail\n",
      "has a black upper tail\n",
      "has a white upper tail\n",
      "has a red upper tail\n",
      "has a buff upper tail\n",
      "has a spotted head pattern\n",
      "has a malar head pattern\n",
      "has a crested head pattern\n",
      "has a masked head pattern\n",
      "has a unique head pattern\n",
      "has a eyebrow head pattern\n",
      "has a eyering head pattern\n",
      "has a plain head pattern\n",
      "has a eyeline head pattern\n",
      "has a striped head pattern\n",
      "has a capped head pattern\n",
      "has a blue breast color\n",
      "has a brown breast color\n",
      "has a iridescent breast color\n",
      "has a purple breast color\n",
      "has a rufous breast color\n",
      "has a grey breast color\n",
      "has a yellow breast color\n",
      "has a olive breast color\n",
      "has a green breast color\n",
      "has a pink breast color\n",
      "has an orange breast color\n",
      "has a black breast color\n",
      "has a white breast color\n",
      "has a red breast color\n",
      "has a buff breast color\n",
      "has a blue throat color\n",
      "has a brown throat color\n",
      "has a iridescent throat color\n",
      "has a purple throat color\n",
      "has a rufous throat color\n",
      "has a grey throat color\n",
      "has a yellow throat color\n",
      "has a olive throat color\n",
      "has a green throat color\n",
      "has a pink throat color\n",
      "has a orange throat color\n",
      "has a black throat color\n",
      "has a white throat color\n",
      "has a red throat color\n",
      "has a buff throat color\n",
      "has a blue eye color\n",
      "has a brown eye color\n",
      "has a purple eye color\n",
      "has a rufous eye color\n",
      "has a grey eye color\n",
      "has a yellow eye color\n",
      "has a olive eye color\n",
      "has a green eye color\n",
      "has a pink eye color\n",
      "has a orange eye color\n",
      "has a black eye color\n",
      "has a white eye color\n",
      "has a red eye color\n",
      "has a buff eye color\n",
      "has a about the same as head bill length\n",
      "has a longer than head bill length\n",
      "has a shorter than head bill length\n",
      "has a blue forehead color\n",
      "has a brown forehead color\n",
      "has a iridescent forehead color\n",
      "has a purple forehead color\n",
      "has a rufous forehead color\n",
      "has a grey forehead color\n",
      "has a yellow forehead color\n",
      "has a olive forehead color\n",
      "has a green forehead color\n",
      "has a pink forehead color\n",
      "has a orange forehead color\n",
      "has a black forehead color\n",
      "has a white forehead color\n",
      "has a red forehead color\n",
      "has a buff forehead color\n",
      "has a blue under tail color\n",
      "has a brown under tail color\n",
      "has a iridescent under tail color\n",
      "has a purple under tail color\n",
      "has a rufous under tail color\n",
      "has a grey under tail color\n",
      "has a yellow under tail color\n",
      "has a olive under tail color\n",
      "has a green under tail color\n",
      "has a pink under tail color\n",
      "has a orange under tail color\n",
      "has a black under tail color\n",
      "has a white under tail color\n",
      "has a red under tail color\n",
      "has a buff under tail color\n",
      "has a blue nape color\n",
      "has a brown nape color\n",
      "has a iridescent nape color\n",
      "has a purple nape color\n",
      "has a rufous nape color\n",
      "has a grey nape color\n",
      "has a yellow nape color\n",
      "has a olive nape color\n",
      "has a green nape color\n",
      "has a pink nape color\n",
      "has a orange nape color\n",
      "has a black nape color\n",
      "has a white nape color\n",
      "has a red nape color\n",
      "has a buff nape color\n",
      "has a blue belly color\n",
      "has a brown belly color\n",
      "has a iridescent belly color\n",
      "has a purple belly color\n",
      "has a rufous belly color\n",
      "has a grey belly color\n",
      "has a yellow belly color\n",
      "has a olive belly color\n",
      "has a green belly color\n",
      "has a pink belly color\n",
      "has a orange belly color\n",
      "has a black belly color\n",
      "has a white belly color\n",
      "has a red belly color\n",
      "has a buff belly color\n",
      "has a rounded-wings wing shape\n",
      "has a pointed-wings wing shape\n",
      "has a broad-wings wing shape\n",
      "has a tapered-wings wing shape\n",
      "has a long-wings wing shape\n",
      "has a large size\n",
      "has a small size\n",
      "has a very large size\n",
      "has a medium size\n",
      "has a very small size\n",
      "has a upright-perching water-like shape\n",
      "has a chicken-like-marsh shape\n",
      "has a long-legged-like shape\n",
      "has a duck-like shape\n",
      "has a owl-like shape\n",
      "has a gull-like shape\n",
      "has a hummingbird-like shape\n",
      "has a pigeon-like shape\n",
      "has a tree-clinging-like shape\n",
      "has a hawk-like shape\n",
      "has a sandpiper-like shape\n",
      "has a upland-ground-like shape\n",
      "has a swallow-like shape\n",
      "has a perching-like shape\n",
      "has a solid back pattern\n",
      "has a spotted back pattern\n",
      "has a striped back pattern\n",
      "has a multi-colored back pattern\n",
      "has a solid tail pattern\n",
      "has a spotted tail pattern\n",
      "has a striped tail pattern\n",
      "has a multi-colored tail pattern\n",
      "has a solid belly pattern\n",
      "has a spotted belly pattern\n",
      "has a striped belly pattern\n",
      "has a multi-colored belly pattern\n",
      "has a blue primary color\n",
      "has a brown primary color\n",
      "has a iridescent primary color\n",
      "has a purple primary color\n",
      "has a rufous primary color\n",
      "has a grey primary color\n",
      "has a yellow primary color\n",
      "has a olive primary color\n",
      "has a green primary color\n",
      "has a pink primary color\n",
      "has a orange primary color\n",
      "has a black primary color\n",
      "has a white primary color\n",
      "has a red primary color\n",
      "has a buff primary color\n",
      "has a blue leg color\n",
      "has a brown leg color\n",
      "has a iridescent leg color\n",
      "has a purple leg color\n",
      "has a rufous leg color\n",
      "has a grey leg color\n",
      "has a yellow leg color\n",
      "has a olive leg color\n",
      "has a green leg color\n",
      "has a pink leg color\n",
      "has a orange leg color\n",
      "has a black leg color\n",
      "has a white leg color\n",
      "has a red leg color\n",
      "has a buff leg color\n",
      "has a blue bill color\n",
      "has a brown bill color\n",
      "has a iridescent bill color\n",
      "has a purple bill color\n",
      "has a rufous bill color\n",
      "has a grey bill color\n",
      "has a yellow bill color\n",
      "has a olive bill color\n",
      "has a green bill color\n",
      "has a pink bill color\n",
      "has a orange bill color\n",
      "has a black bill color\n",
      "has a white bill color\n",
      "has a red bill color\n",
      "has a buff bill color\n",
      "has a blue crown color\n",
      "has a brown crown color\n",
      "has a iridescent crown color\n",
      "has a purple crown color\n",
      "has a rufous crown color\n",
      "has a grey crown color\n",
      "has a yellow crown color\n",
      "has a olive crown color\n",
      "has a green crown color\n",
      "has a pink crown color\n",
      "has a orange crown color\n",
      "has a black crown color\n",
      "has a white crown color\n",
      "has a red crown color\n",
      "has a buff crown color\n",
      "has a solid wing pattern\n",
      "has a spotted wing pattern\n",
      "has a striped wing pattern\n",
      "has a multi-colored wing pattern\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for item in CUBattributeclasses:\n",
    "    i=i+1\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad84f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CUBattributeclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52190881",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "inputs = tokenizer([(f\"This bird {c}\") for c in CUBattributeclasses], padding=True, return_tensors=\"pt\")\n",
    "txt_features = CLIP.get_text_features(**inputs)\n",
    "txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "txt_features = txt_features.float()\n",
    "\n",
    "np.save('CUB312text_feature.npy',txt_features.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features, _ = clip.load('ViT-L/14@336px', device) \n",
    "CoCoclasses=['person','bicycle','car', 'motorcycle', 'airplane', 'bus', 'train','truck',\n",
    "         'boat','traffic light','fire hydrant','stop sign','parking meter','bench',\n",
    "         'bird','cat','dog','horse','sheep','cow','elephant','bear','zebra','giraffe',\n",
    "         'backpack','umbrella','handbag','tie','suitcase','frisbee','skis','snowboard',\n",
    "         'sports ball','kite','baseball bat','baseball glove','skateboard','surfboard',\n",
    "         'tennis racket','bottle','wine glass','cup','fork','knife','spoon','bowl',\n",
    "         'banana','apple','sandwich','orange','broccoli','carrot','hot dog','pizza',\n",
    "         'donut','cake','chair','couch','potted plant','bed','dining table','toilet',\n",
    "         'tv','laptop','mouse','remote','keyboard','cell phone','microwave','oven',\n",
    "         'toaster','sink','refrigerator','book','clock','vase','scissors','teddy bear',\n",
    "         'hair drier','toothbrush']\n",
    "CLIP = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "inputs = tokenizer(CoCoclasses, padding=True, return_tensors=\"pt\")\n",
    "txt_features = CLIP.get_text_features(**inputs)\n",
    "txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "txt_features = txt_features.float()\n",
    "\n",
    "np.save('CoCo80text_feature_labelonly.npy',txt_features.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61f5c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.available_models()\n",
    "device = \"cpu\"\n",
    "clip_model, preprocess = clip.load('ViT-L/14@336px', device)\n",
    "ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "\n",
    "\n",
    "n_ctx=4\n",
    "ctx_dim=768\n",
    "ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=torch.float32)\n",
    "nn.init.normal_(ctx_vectors, std=0.02)\n",
    "prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "\n",
    "ctx = nn.Parameter(ctx_vectors)\n",
    "if ctx.dim() == 2:\n",
    "    ctx = ctx.unsqueeze(0).expand(80, -1, -1)\n",
    "        \n",
    "classnames=['person','bicycle','car', 'motorcycle', 'airplane', 'bus', 'train','truck',\n",
    "         'boat','traffic light','fire hydrant','stop sign','parking meter','bench',\n",
    "         'bird','cat','dog','horse','sheep','cow','elephant','bear','zebra','giraffe',\n",
    "         'backpack','umbrella','handbag','tie','suitcase','frisbee','skis','snowboard',\n",
    "         'sports ball','kite','baseball bat','baseball glove','skateboard','surfboard',\n",
    "         'tennis racket','bottle','wine glass','cup','fork','knife','spoon','bowl',\n",
    "         'banana','apple','sandwich','orange','broccoli','carrot','hot dog','pizza',\n",
    "         'donut','cake','chair','couch','potted plant','bed','dining table','toilet',\n",
    "         'tv','laptop','mouse','remote','keyboard','cell phone','microwave','oven',\n",
    "         'toaster','sink','refrigerator','book','clock','vase','scissors','teddy bear',\n",
    "         'hair drier','toothbrush']\n",
    "\n",
    "prompts = [name + \".\" for name in classnames]\n",
    "tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92851a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "if ctx.dim() == 2:\n",
    "    ctx = ctx.unsqueeze(0).expand(80, -1, -1)\n",
    "ctx.shape\n",
    "a=tokenized_prompts.argmax(dim=-1)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e184a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031991004943847656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 4519,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d0363852e540c2967de9dcba87f1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/4.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03154468536376953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 1710671599,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f188e10eabea427993cefee8c018fc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029670000076293945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.json",
       "rate": null,
       "total": 961143,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc2227f544544cf8d17ff098d43541a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/939k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.028396129608154297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading merges.txt",
       "rate": null,
       "total": 524619,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd910a32b424a86b03ff26a2b78ec0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/512k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.11691761016845703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading special_tokens_map.json",
       "rate": null,
       "total": 389,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ee63094f8647faa78ad6fc56491492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.030475854873657227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 905,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46166ff6783a483192bcfb53322574cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CLIP = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "inputs = tokenizer([(f\"a photo of a {c}\") for c in CoCoclasses], padding=True, return_tensors=\"pt\")\n",
    "txt_features = CLIP.get_text_features(**inputs)\n",
    "txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "txt_features = txt_features.float()\n",
    "\n",
    "np.save('CoCo80text_feature_224.npy',txt_features.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c95a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "inputs = tokenizer([(f\"a photo of a {c}\") for c in CoCoclasses], padding=True, return_tensors=\"pt\")\n",
    "txt_features = CLIP.get_text_features(**inputs)\n",
    "txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "txt_features = txt_features.float()\n",
    "\n",
    "np.save('CoCo80text_feature_vitb16_224.npy',txt_features.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa230108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPModel(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (position_embedding): Embedding(77, 512)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "      (position_embedding): Embedding(197, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "CLIP = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "print(CLIP)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "inputs = tokenizer([(f\"a photo of a {c}\") for c in CoCoclasses], padding=True, return_tensors=\"pt\")\n",
    "txt_features = CLIP.get_text_features(**inputs)\n",
    "txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "txt_features = txt_features.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f4a3d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b3c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "inputs = tokenizer([(f\"a photo of a {c}\") for c in CoCoclasses], padding=True, return_tensors=\"pt\")\n",
    "txt_features = CLIP.get_text_features(**inputs)\n",
    "txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "txt_features = txt_features.float()\n",
    "\n",
    "np.save('CoCo80text_feature_vitb32_224.npy',txt_features.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5cc9b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e674c108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_projection.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'visual_projection.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'logit_scale', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "features = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061cc24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
